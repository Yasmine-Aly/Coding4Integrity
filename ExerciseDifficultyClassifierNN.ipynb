{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjvEGWk/n7JU97kop7fg12",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lynaBoukari/Coding4Integrity/blob/main/ExerciseDifficultyClassifierNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise Difficulty Classification Model\n",
        "\n",
        "In this section, we implement a neural network model to classify exercises into different difficulty levels (easy, medium, or hard). This model plays a crucial role in automating the process of exercise metadata updates.\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "The neural network is a feedforward neural network with the following layers:\n",
        "\n",
        "- Input Layer: This layer takes the input features of exercises as input.\n",
        "- Hidden Layers: These layers process the input data through a series of weighted connections and activation functions.\n",
        "- Output Layer: This layer produces the predicted difficulty level.\n",
        "\n",
        "### Training and Evaluation\n",
        "\n",
        "The model is trained on a labeled dataset of exercises, where each exercise is associated with a difficulty level. The model learns to map input features to the corresponding difficulty level during training.\n",
        "\n",
        "After training, the model's performance is evaluated on a separate test dataset to assess its accuracy and generalization capabilities.\n",
        "\n",
        "### Usage\n",
        "\n",
        "The trained model is integrated into the exam paper platform, where it automatically classifies exercises based on their features and updates the exercise metadata with the predicted difficulty level.\n",
        "\n",
        "### Implementation Details\n",
        "\n",
        "- Dataset: The training data consists of exercises with labeled difficulty levels. The data is preprocessed before feeding it into the neural network.\n",
        "- Training: The model is trained using a training set and optimized using backpropagation and gradient descent.\n",
        "- Evaluation: The model's performance is assessed on a separate test set using metrics such as accuracy.\n",
        "\n",
        "### Code Implementation\n",
        "\n",
        "The following code cells demonstrate how to implement and train the neural network model for exercise difficulty classification."
      ],
      "metadata": {
        "id": "aSwdl1fyeIDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Load your exercise difficulty data from your CSV file\n",
        "data = pd.read_csv('/content/level_difficulty.csv')\n",
        "\n",
        "# Encode the difficulty labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "data['difficulty'] = label_encoder.fit_transform(data['difficulty'])\n",
        "\n",
        "# Tokenize the exercise text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['description'])\n",
        "X = tokenizer.texts_to_sequences(data['description'])\n",
        "X = pad_sequences(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['difficulty'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a simple neural network model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X.shape[1]))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=3, activation='softmax'))  # 3 output classes (Easy, Medium, Hard)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cH05cNBTGBh",
        "outputId": "c0052ea7-6cbb-4706-b7fd-91d14dd5a626"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 3s 670ms/step - loss: 1.0390 - accuracy: 0.6585 - val_loss: 0.9164 - val_accuracy: 0.7619\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.8523 - accuracy: 0.7805 - val_loss: 0.7977 - val_accuracy: 0.7619\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.7319 - accuracy: 0.7805 - val_loss: 0.7416 - val_accuracy: 0.7619\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.6569 - accuracy: 0.7805 - val_loss: 0.7326 - val_accuracy: 0.7619\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.5958 - accuracy: 0.7805 - val_loss: 0.7248 - val_accuracy: 0.7619\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.5383 - accuracy: 0.7805 - val_loss: 0.7102 - val_accuracy: 0.7619\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.4734 - accuracy: 0.7805 - val_loss: 0.6857 - val_accuracy: 0.7619\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.4168 - accuracy: 0.8537 - val_loss: 0.6665 - val_accuracy: 0.7143\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3656 - accuracy: 0.8780 - val_loss: 0.6519 - val_accuracy: 0.7143\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3217 - accuracy: 0.8902 - val_loss: 0.6401 - val_accuracy: 0.7143\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.5516 - accuracy: 0.8077\n",
            "Test loss: 0.5516, Test accuracy: 0.8077\n"
          ]
        }
      ]
    }
  ]
}